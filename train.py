import tensorflow as tf
import numpy as np
from Unet import UNet
from discriminator import Discriminator
import os
from PIL import Image
import random
import time 

class Train():
    def __init__(self):

        #realA RGB
        self.realA = tf.placeholder(tf.float32, shape=[None,388,388,3])
        
        #realB 線画
        self.reshaped_realB = tf.placeholder(tf.float32, shape=[None,388,388,3])
        self.realB = tf.placeholder(tf.float32, shape=[None,572,572,3])

        #batch_size
        batch_size = self.realA.get_shape().as_list()[0]
        
        #Generated by UNet used realB
        #fakeA 着色
        self.fakeA = UNet(self.realB).dec_conv_last

        #concat
        #positive
        #realAB
        realAB = tf.concat([self.realA, self.reshaped_realB], 3)        
        #negative
        #fakeAB
        fakeAB = tf.concat([self.fakeA, self.reshaped_realB], 3)
        
        #discriminator
        dis_real = Discriminator(realAB, batch_size)
        real_logits = dis_real.last_h
        real_out = dis_real.out

        dis_fake = Discriminator(fakeAB, batch_size)
        fake_logits = dis_fake.last_h
        fake_out = dis_fake.out

        self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_out)))
        self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_out)))
        self.UNet_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_out)))

        self.d_loss = self.d_loss_fake + self.d_loss_real

        self.opt_d = tf.train.AdamOptimizer(0.0003).minimize(self.d_loss)
        self.opt_g = tf.train.AdamOptimizer(0.0003).minimize(self.UNet_loss)

batch_size = 5
epochs = 3000
filenames = os.listdir('./data/rgb388/')
data_size = len(filenames)
step = int(data_size/batch_size)

def sample(size, channel, path, batch_files):
    imgs = np.empty((0,size,size,channel), int)

    for file_name in batch_files:
        img = np.array(Image.open(path+file_name))
        #print(imgs.shape,img.shape) 
        imgs = np.append(imgs, np.array([img]), axis=0)
    imgs = imgs.reshape((-1,size,size,channel))
    return imgs

#batch_files = [random.choice(filenames) for _ in range(batch_size)]  
#sample(size=388, channel=3, path='./data/linedraw388/', batch_files=batch_files)


train = Train()
with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:
    tf.global_variables_initializer().run()
    saver = tf.train.Saver(tf.global_variables())
    
    for epoch in range(epochs):
        new_time = time.time() 
        sum_g_loss = 0.
        sum_d_loss = 0.
        for _ in range(0, data_size, batch_size):
            batch_files = [random.choice(filenames) for _ in range(batch_size)]
            
            rgb398 = sample(388, 3, './data/rgb388/', batch_files)
            linedraw398 = sample(388, 3, './data/linedraw388/', batch_files)
            linedraw572 = sample(572, 3, './data/linedraw572/', batch_files)

            g_loss, _ =sess.run([train.UNet_loss,train.opt_g], {train.realA:rgb398,train.reshaped_realB:linedraw398,train.realB:linedraw572})
            d_loss, _ =sess.run([train.d_loss,train.opt_d], {train.realA:rgb398,train.reshaped_realB:linedraw398,train.realB:linedraw572}) 

            sum_g_loss+= g_loss
            sum_d_loss+= d_loss

        print('epoch_num:'+epoch+'    g_loss:'+(sum_g_loss/step)+'    d_loss:'+(sum_d_loss/step)+' speed:'+time.time()-new_time)
        saver.save(sess, "saved/model.ckpt")
